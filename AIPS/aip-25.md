<pre>
  AIP: 0025
  Title: Providing PoW like guarantees on (D)PoS networks
  Authors: Moazzam Abdullah Khan, Asif Mehmood
  Status: Draft
  Type: Standards Track
  Discussion: https://github.com/ArkEcosystem/AIPs/issues/39
  Created: 2018-12-08
  Last Update: 2019-1-10
</pre>


Abstract
========

Traditional Proof of Stake (PoS) and Delegated Proof of Stake (DPoS) approaches achieve Probabilistic Byzantine Fault Tolerant consensus without creating the race condidtion that Proof of Work (PoW) systems create. As a result these approaches waste less energy and therefore are able to provide utility at a cheaper cost. However one drawback of conventional (D)PoS approaches is that they cannot guarantee data immutability as a result of predictable hash sequences or easy re-sync guarantees by accepting only the longest chain as valid. This proposal aims to address these concerns by introducing an approach to determine producer of the next block while keeping them hidden until the block is actually announced. This enables strong guarantees that the data is immutable and that the next block's forger isn't known to the network beforehand thereby eliminatng a DDoS attack vector.


Motivation
==========

As cryptocurrencies get used more widely it is desired that the infrastructure and algorithms required to deploy them become more cost efficient so that barriers to entry for aspiring developers and dapp creators are reduced. This desire has given rise to PoS and DPoS paradigms which achieve the same level of concensus as PoW systems at several orders of magnitude lower cost. However these (D)PoS systems don't provide a hard guarantee that data committed to the blockchain cannot be changed. As a result (D)PoS systems cannot take advantage of the longest chain valid principle that creates concensus in PoW systems because doing would make them vulnerable to deep reorg attacks. This kind of attack is undetectable in current schemes and is pretty cheap to carry out. Because of this failure there is no easy way to trust data provided by other nodes and local backups of the blockchain have to be relied upon for a guarantee that the data has not been changed for older blocks. These are critical issues that need to be addressed and one approach that we can take to solve them is discussed below. This proposal will describe the approach in context to Ark but it can be applied to any Proof of Stake system.


Rationale
=========

The main emphasis in this approach is to create a verifiable random function (VRF) that is used to generate random numbers. This allows us to select the next block producer using the generated random number. This is achieved by making each delegate submit a pledge containing the hash of a secret random number. If the random number generated by the VRF is concurrent with the pledged secret submitted by a delegate then that particular delegate is allowed to create the next block. While creating the new block the delegate will add the secret random number as claim corresponding to the colliding pledge as proof for other delegates to validate their claim of having the right to produce the block.

One issue that must be pointed out while working with VRF implementations is the last actor problem which is a situation in which the latest block producer can keep producing blocks and discarding them until they have found one that confirms themself to be the next block producer as well. In order to circumvent the issue altogether we mandate that the VRF is created from blocks at least N depth away from current height H of the blockchain where N is the total number of forging delegates.

1. Bootstrapping process
	* Randomized Round robin for 2 rounds using current approach implemented by the Ark v2 protocol
	* During this round with each block the producer of the block commits the sha256 hash of a string containing a large secret number and time epoch to the blockchain via a special pledge transaction (discussed below)
	
2. Creating the VRF
	* The hashes pledged by the delegates in the blocks from depth N to 2*N from current block height H are collected and concatenated into a string. This string is then hashed using sha256 to create the new random number.  Using this approach all N delegates get an equal chance to contribute to the VRF's next output and there is no chance for hijacking the random number generator by a minority of colluding parties.

3. Selecting Next Blockproducer
	* The new random number generated is modded by a number C smaller than N
	* Each delegate mods their secret number by C as well
	* If both of these resulting numbers match for a delegate then that delegate has the right to forge next block
	* Other nodes will only accept block produced for which the claim correlates to the earliest unused pledge and will reject claim with a later pledge committed in a later block
	* The block produced by the chosen delegate will contain the claim which will reveal the secret random number and the salt used before hashing it. The salt in this case is the unix time epoch and it must be close to the transaction creation time for the claim to be considered valid.

Using this approach it is possible that multiple delegates have submitted numbers that produce the same remainder when modded with C or that no  delegates have submitted a number that corresponds with the current VRF output. These cases must be handled specifically in the protocol.
 
1. If more than one delegate have submitted pledges that result in the same remainder then the unused pledge that was submitted in the earliest block will be used. Delegates can estimate a risk factor for producing a block that will be orphaned based on the depth of their pledge being used as claim. Using this calculation a delegate can decide to immediately publish a block or wait roughly 2 seconds so it can check if someone else has already published a block, if not it can publish it.

2. In case no blocks are heard after a reasonable amount of time (let's say ~4 seconds) it can be assumed that no one has submitted the pledge corresponding to the current VRF output. In order to make sure the chain doesn't stall a sha256 of the VRF output is calculated and is used as collision target for next block producer. This process is repeated until a block is produced and the normal mechanism for VRF calculation is resumed afterwards. The block will only be accepted by peers in the corresponding time window (~4 seconds as described above), once the time window is expired it will not be accepted. The time span for each window must be defined keeping in mind the number of delegates and should be roughly longer than the time it takes for 51% of the nodes in the network to receive a new block.

An additional constraint we add is that each delegate can only have up to 3 pledges active at a time. If more than 3 are submitted then only the latest 3 will be considered valid.

Specifications
==============

The pseudocode for a forging node is presented below with the forge() function being entrypoint:

```
inputs:
N = number of delegates (or minimum number of PoS miners);
C = integer less than N;
D = dictionary to store hash and pledge secret locally
unclaimed_block_hash = hash corresponding to block if the claim transaction is ignored

algorithm:
function create_pledge()
{
	rand = random.next()
	salt = unix_time_epoch
	public_rand = sha256(string(rand)+string(salt))
	return public_rand, rand, salt
}

function forge()
{
	if no_active_pledge:
		public_rand, rand, salt = create_pledge()
		D.add_item(public_rand, {rand, salt})
		publish_pledge_tx(public_rand)

	if H > 2*N:
	{
		for each pledge_i in D below height H-N:
		{
			j = sha256( concatenate_hashes_from_height(H-N, H-2*N) ) % C
			if D.at(pledge_i).rand%C == j:
			{
				D.remove(pledge_i)
				public_rand, rand, salt = create_pledge()
				D.add_item(public_rand, {rand, salt})
				publish_pledge_tx(public_rand)
				publish_claim_tx(pledge_i, unclaimed_block_hash)
				create_next_block()
				break;
			}
		
		}
	}
	else
	{
		follow_old_algorithm()
	}
}
```

The pledge and claim transactions are a new transaction type with a dynamic fee defined to avoid a nothing at stake condition for bidding on competing forks of the same chain. If a delegate produces a block that is uncled then all relays extract the pledge and claim transactions specified within it and add it to the mempool to be forged by the next block producer. The transaction fee would be selected based on a tradeoff between uncle rate and block time delay. If there are too many uncle blocks being produced then the transaction fee should be increased to penalize the delegates producing uncled blocks and change their risk/reward compromize. However if this fee is too high then delegates might be reluctant to produce blocks even if they have a valid claim simply because they fear that someone else might have an earlier claim and therefore delay the production of next block unnecessarily.

Pledge transaction example

| type | data |
|-|-|
| 0xfe | (public_rand) |

Claim transaction example

| type | data |
|-|-|
| 0xff | (rand):(salt):(unclaimed_block_hash) |

`unclaimed_block_hash` is what the hash of the block would have been if the claim transaction wasn't included in the block data (excluding signature). This is meant to discourage double forging by producing competing blocks using the same claim that lead to a fork.

Finally the delegate can assess their risk of having the block rejected due to earlier claim based on revealed claims and calculating probability using the [birthday problem formula](https://en.wikipedia.org/wiki/Birthday_problem) and estimate their profitability using [kelly criterion](https://en.wikipedia.org/wiki/Kelly_criterion).

In this algorithm the delegates can tweak the fees for the two new transaction types as well as the variable C to get the best network performance versus uncle rate balance. For this to happen we propose that the block also specify a suggested value for C and the median value from the last N blocks is used to calculate next block.

Addressing Long Range Attacks
=====
A significant issue that stops (D)PoS networks from following the longest chain rule is known as a Long Range Attack. In this attack adverserial actors that used to hold stake in the network in the past can collude if they had a majority anytime in the past and create a new fork from that point. This group of nodes can then selectively replay transaction in the newly created Long Range Chain (LRC) and keep adding blocks until it overtakes the original Honest Chain (HC). Since there's no proof of work involved the LRC doesn't have any limitations in how many blocks can be added per second therefore the chance of a (D)PoS network following the longest chain will have an increased chance of getting long range attacked as time passes and old stake holders or delegates sell their stake. Since these old stake holders have already sold their stake they have no disincentive that prevents them from considering to attack the honest chain. In this section we discuss two differen approaches that may be used in defending against such attacks.

### Approach 1
One method that might be viable to protect against getting long range attacked is to set a hard depth limit beyond which a fork will not be considered for selecting the longest chain. This means that the delegates will calculate which block was the last common block between the two forks and only switch if the block is not older than the last 40 blocks for example. The depth limit of 40 is chosen here since it allows enough time for fork resolution to happen near the head of the chain but isn't far enough in the past where some exchanges would be affected. While this approach may protect exchanges, merchants using Ark as their payment gateway might not wait for 40 block times before handing out a product. One solution to ensure they can still trust the network and don't get double spent is to implement zero confirmation transactions.

Zero confirmation (0 conf) transactions can only be trusted if the network always has capacity to include a broadcasted transaction in the next block and if the network is forced to prioritize transactions based on the time they were received by the network rather than by the transaction fees. Additionally merchant solutions will often have full nodes or relays installed that will detect if a double spend was attempted and delay the order if one is detected until the chain settles. Generally the time needed to wait and confirm that a double spend wasn't attempted is 3 seconds since that's the time required for enough nodes in the network to propagate transactions so that the competing transaction can't overtake the network. As mentioned before 0 conf transactions require that the transaction can be included in the next block which means that the max block size has to be increased past what's required to accommodate the expected peak number of transactions. Finally the forging nodes must communicate, with other nodes in the network, each transaction they receive and signal if a double spend was detected so that every node in the network knows of the competing transaction.

### Approach 2
If for some reason it is decided that the above mentioned approach is not suitable for adoption in Ark then another approach can be considered in which instead of having an abrupt fork depth limit we can spread out the limit based on probabilistic acceptance as a function of depth of common block. This approach takes inspiration from how frequently forks of different depths occur on the Bitcoin network and the fact that they seem to follow an exponential decay pattern i.e allegedly a 1 block depth fork happens on the Bitcoin network every day, 2 block depth fork happens every week and a 6 block fork is expected to happen once every 100 years. The only difference in our approach would be that a certain small percentage of the network would be expected to be on a minority fork for a given block depth. We can specify how the distribution of this network split looks like by declaring as part of the protocol how the acceptance probability of a competing chain changes with depth. It is suggested that the probability function should take the form 

`p_accept = 1/(A^(fork_depth+1))`

where p_accept describes the weight with which the competing chain will be considered for acceptance in the previously mentioned kelly criterion formula to decide which fork the delegate should bid on. In the above formula A is a constant specified by the delegates and fork_depth describes the depth from current height where the last common block was located in the honest chain. When the kelly outputs for both competing forks have been calculated the node would choose one randomly with their relative weights. An additional constraint is that the reevaluation to accept a competing fork happens at regular intervals according to the node's internal clock instead of being triggered by outside events such as receiving a new block etc. This ensures that delegates can easily resolve forks occurring at the head of the chain in a probabilistic manner similar to PoW networks but the chance that a long range attack can be successful in taking over the whole network is so small that it can be expected not to happen within the lifetime of the universe. Some additional points that have to be followed when using this approach are that exchanges and wallets need to poll multiple different nodes in order to be satisfied that they got the correct state from the nodes. The probability that they get a bad state follows the same exponential decay formula and if 1% of the network's nodes agree on a certain state then it's safe to trust it as long as the required transaction happened before last `fork_depth` blocks corresponding to `p_accept=1%`. To make sure that a node can recover from choosing a bad fork they can offset the acceptance probability based on the number of blocks they have forged on the competing chain (which would be the honest chain here) in the past with decaying weight. For instance if a node randomly accepts a competing fork with common block at depth 30, however unlikely that may be, then it will be more likely to switch back because it will have more blocks that were forged by the delegate in question on the old honest chain than in the new long range attack chain. This would however necessitate that the blocks received by new delegates while syncing are on the honest chain and not on the long range attack chain.

Finally it must be stated that both of the approaches presented above assume that the network isn't under sybil attack. Defending against sybil attacks is a topic that we will try to address at another time.